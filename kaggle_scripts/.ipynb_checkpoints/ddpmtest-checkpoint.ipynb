{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "cf8d14c8-7fb2-4bdd-8adb-c074cd08195b",
    "_uuid": "4b133fd3-359b-4334-a65a-ff4a53d91d1f",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/home/ec2-user/DiffusionPokemonProject/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusionpokemon.models.autoencoder_blocks import TimeEmbedding\n",
    "from diffusionpokemon.models.ddpm_unet import DDPMModel, DDPMUNet\n",
    "from diffusionpokemon.utils.callbacks import SampleCallback, DenoiseMidwaySampleCallback\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping, \n",
    "    StochasticWeightAveraging, \n",
    "    Callback, \n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "\n",
    "torch.manual_seed(314)\n",
    "torch.cuda.manual_seed_all(314)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## Define hyperparameters\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      5\u001b[39m ON_KAGGLE = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "## Define hyperparameters\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "ON_KAGGLE = False\n",
    "if not load_dotenv():\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    os.environ[\"WANDB_API_KEY\"] = UserSecretsClient().get_secret(\"wandb_api\")\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = UserSecretsClient().get_secret(\"s3_aws_access_key\")\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = UserSecretsClient().get_secret(\"s3_aws_secret_access_key\")\n",
    "    ON_KAGGLE = True\n",
    "    \n",
    "import wandb\n",
    "\n",
    "project_name = PROJECT_NAME = \"test-ddpm-lightning\"\n",
    "\n",
    "if config_json_filename := os.getenv(\"CONFIG_JSON_NAME\", None):\n",
    "    with open(config_json_filename, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "else:\n",
    "    config = dict(\n",
    "        batch_size = 128,\n",
    "        use_constant_lr = False,\n",
    "        lr = 1e-4,\n",
    "        num_epoch = 1_000,\n",
    "        dropout = 0.0,\n",
    "        overfit_batch = 0.0,\n",
    "        weight_decay = 0.0,\n",
    "        log_wandb = True,\n",
    "        init_new_wandb_run = False,\n",
    "        use_existing_run = \"setviymk\",\n",
    "        use_augmentation = True,\n",
    "        accumulate_grad_batches = 1,\n",
    "        ddpm__n_steps = 1_000,\n",
    "        ddpm__input_image_channels = 3,\n",
    "        ddpm__input_image_size = 32,\n",
    "        unet__channels_mult = [1, 4, 4, 4, 4, 4],\n",
    "        unet__is_attn = [False, False, False, True, True, True],\n",
    "        unet__n_blocks = 12,\n",
    "        unet__hidden_dim = 64,\n",
    "        unet__use_conv_for_res_change = True,\n",
    "        is_finetune = False,\n",
    "        lr_scheduler__class = \"CosineAnnealingLR\",\n",
    "        max_trainer_time_limit = \"00:20:00:00\",\n",
    "    )\n",
    "\n",
    "for k, v in config.items():\n",
    "    if k not in globals() or globals()[k] != v:\n",
    "        globals()[k] = v\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "if log_wandb:\n",
    "    wandb.login()\n",
    "\n",
    "    if \"run\" not in globals():\n",
    "        run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            id=use_existing_run if (use_existing_run and not init_new_wandb_run) else None,\n",
    "            resume=\"must\" if (use_existing_run and not init_new_wandb_run) else None,\n",
    "            config={\n",
    "                **config,\n",
    "                \"mode\": \"offline\" if not log_wandb else \"online\"\n",
    "            }\n",
    "        )\n",
    "        assert run is not None\n",
    "\n",
    "# some common things\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "def make_normaliser(num_channels=1):\n",
    "    return transforms.Normalize([0.5]*num_channels, [1]*num_channels) # - 0.5 / 1\n",
    "\n",
    "def make_inv_normaliser(num_channels=1):\n",
    "    return transforms.Compose([transforms.Normalize([-0.5]*num_channels, [1]*num_channels), lambda x: torch.clip(x, 0, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef17ba2d-be09-4599-8f85-dc8d586c3504",
    "_uuid": "f7d78736-dd66-4872-afb7-a12a6eadf9e7"
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "839bf320-61d4-427d-9137-220730ab2854",
    "_uuid": "8c0f1b55-be38-4ecd-92ca-92e299a7dc8e",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cifar_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    make_normaliser(),\n",
    "    *(\n",
    "        [] \n",
    "        if not config[\"use_augmentation\"] \n",
    "        else [\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "        ] \n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"cifar\", train=True, transform=cifar_transforms, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"cifar\", train=False, transform=cifar_transforms, download=True)\n",
    "\n",
    "# # Filter indices where label == 0 (airplane)\n",
    "# plane_train_indices = [i for i, (_, label) in enumerate(train_dataset) if label == 0]\n",
    "# plane_test_indices = [i for i, (_, label) in enumerate(test_dataset) if label == 0]\n",
    "\n",
    "# # Create subsets of only airplanes\n",
    "# plane_train_set = Subset(train_dataset, plane_train_indices)\n",
    "# plane_test_set = Subset(test_dataset, plane_test_indices)\n",
    "\n",
    "# mnist_transforms = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     make_normaliser(),\n",
    "# ])\n",
    "# train_dataset = datasets.MNIST(root=\"mnist\", train=True, download=True, transform=mnist_transforms)\n",
    "# test_dataset = datasets.MNIST(root=\"mnist\", train=False, download=True, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61a814e1-bfdf-4c9e-97e1-4f71ab5134bc",
    "_uuid": "89cc13cd-df1d-40a0-8945-7fde292b0580",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# split train valid\n",
    "print(f\"Train: {len(train_dataset)}, Valid: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# append steps_per_epoch\n",
    "effective_batch_size = batch_size * (1 if accumulate_grad_batches is None else accumulate_grad_batches)\n",
    "num_train_examples = len(train_dataset) if overfit_batch == 0.0 else (overfit_batch * batch_size)\n",
    "steps_per_epoch = math.ceil(num_train_examples / effective_batch_size)\n",
    "print(\"Number of steps per epoch:\", f\"{steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    }
   },
   "outputs": [],
   "source": [
    "inv_t = make_inv_normaliser()\n",
    "\n",
    "plt.imshow(to_pil(inv_t(test_dataset[12][0])))\n",
    "\n",
    "print(\"Input shape:\", test_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf723c8d-9711-4184-86b5-3b5580ca5ea1",
    "_uuid": "94f988a5-0ba1-44d5-93bd-205107e315a1"
   },
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c680c92-bbe9-4b81-8eca-6e58f976467e",
    "_uuid": "26f00ec1-2d45-4b81-b2a6-be664aafdaeb",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer_kwargs=dict(\n",
    "    use_constant_lr=config[\"use_constant_lr\"],\n",
    "    lr_scheduler__class=config[\"lr_scheduler__class\"],\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "if config[\"lr_scheduler__class\"] == \"CosineAnnealingLR\":\n",
    "    optimizer_kwargs[\"lr_sched_freq__step\"] = 1\n",
    "    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n",
    "        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n",
    "        \"eta_min\": config[\"lr\"] / 100\n",
    "    }\n",
    "elif config[\"lr_scheduler_class\"] == \"ReduceLROnPlateau\":\n",
    "    optimizer_kwargs[\"lr_sched_freq__step\"] = steps_per_epoch\n",
    "    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n",
    "        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n",
    "        \"eta_min\": config[\"lr\"] / 100\n",
    "    }\n",
    "\n",
    "model = DDPMUNet(\n",
    "    n_steps=config[\"ddpm__n_steps\"],\n",
    "    input_size=config[\"ddpm__input_image_size\"],\n",
    "    optimizers_kwarg=optimizer_kwargs,\n",
    "    eps_model_kwargs=dict(\n",
    "        n_blocks=config[\"unet__n_blocks\"], \n",
    "        n_channels=config[\"unet__hidden_dim\"],\n",
    "        channels_mult=config[\"unet__channels_mult\"], \n",
    "        is_attn=config[\"unet__is_attn\"],\n",
    "        res_block_dropout=config[\"dropout\"],\n",
    "        use_conv_for_res_change=config[\"unet__use_conv_for_res_change\"],\n",
    "        input_channels=config[\"ddpm__input_image_channels\"]\n",
    "    ),\n",
    "    is_finetune=False,   # we're not interested in finetuning for a while...\n",
    ").to(device)\n",
    "       \n",
    "print(f\"Number of trainable params: {sum(p.numel() for p in model.eps_model.parameters() if p.requires_grad):,}\")\n",
    "print(\"Model device:\", model.device)\n",
    "\n",
    "print(optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.783Z"
    }
   },
   "outputs": [],
   "source": [
    "# test model forward noising\n",
    "sample_img = train_dataset[13][0]\n",
    "\n",
    "num_imgs = 10\n",
    "ts = torch.as_tensor(np.round(np.linspace(0, model.n_steps-1, num_imgs)), dtype=torch.long).to(model.device)\n",
    "x = torch.unsqueeze(sample_img, 0).expand(num_imgs, *sample_img.shape).to(model.device)\n",
    "true_noise_e = torch.randn_like(x).to(model.device)\n",
    "noised_x_t = model.noise_sample_at_timestep(x, ts, true_noise_e).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=num_imgs, nrows=1, figsize=(num_imgs, 1))\n",
    "for idx, ax in enumerate(axs):\n",
    "    ax.imshow(to_pil(inv_t(noised_x_t[idx])))\n",
    "    ax.grid(False)\n",
    "    ax.axis(False)\n",
    "    ax.set_title(f\"t={str(ts[idx].item())}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.783Z"
    }
   },
   "outputs": [],
   "source": [
    "# test model denoising\n",
    "out = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\n",
    "plt.imshow(to_pil(inv_t(out.cpu()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "251551c5-3a53-4a48-8d44-2ec0ce3d923a",
    "_uuid": "9ada2b4e-0e48-44a8-a2fb-492d949ab702"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1472c65f-f076-4f5a-bc2c-ef60d0d3e958",
    "_uuid": "5bcc4915-e866-4bf9-bbec-fc4526654dc3",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.783Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"train_loss__step\",\n",
    "        min_delta=-float('inf'),       # always accept\n",
    "        patience=5,\n",
    "        verbose=False,\n",
    "        check_finite=True,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "]\n",
    "logger = None\n",
    "\n",
    "validate_every_n_steps = (steps_per_epoch * config[\"num_epoch\"]) // 400\n",
    "\n",
    "if config[\"log_wandb\"]:\n",
    "    logger = WandbLogger(project=project_name, prefix=\"CIFAR\")\n",
    "    \n",
    "    try:\n",
    "        logger.watch(model)\n",
    "    except ValueError as e:\n",
    "        if \"You can only call `wandb.watch` once per model.\" not in str(e):\n",
    "            raise e\n",
    "            \n",
    "    callbacks.append(\n",
    "        SampleCallback(\n",
    "            logger=logger, \n",
    "            inv_normaliser=make_inv_normaliser(), \n",
    "            every_n_steps=validate_every_n_steps, \n",
    "            batch_size=4,\n",
    "            input_channels=config[\"ddpm__input_image_channels\"],\n",
    "        )\n",
    "    )\n",
    "    callbacks.append(\n",
    "        DenoiseMidwaySampleCallback(\n",
    "            logger=logger, \n",
    "            seed_img_transformed=test_dataset[12][0],\n",
    "            noise_at_ts=[100, 50, 10],\n",
    "            every_n_steps=validate_every_n_steps,\n",
    "            inv_normaliser=make_inv_normaliser()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    callbacks.append(\n",
    "        ModelCheckpoint(\n",
    "            dirpath=f\"s3://deep-learning-personal-projects/{project_name}/{run.id}/\",\n",
    "            filename=\"{epoch}-{step}--valid_loss={valid_loss__epoch:.3f}\",\n",
    "            monitor=\"valid_loss__epoch\",\n",
    "            save_last=True,\n",
    "            train_time_interval=datetime.timedelta(minutes=45),\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "            enable_version_counter=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    callbacks.append(LearningRateMonitor(logging_interval='step'))\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\" if device == \"cuda\" else \"cpu\", \n",
    "    devices=1, \n",
    "    max_epochs=config[\"num_epoch\"],\n",
    "    log_every_n_steps=config[\"accumulate_grad_batches\"], # if accumulate, reduce logging frequency since we only do one optimisation step after X batches\n",
    "    precision=32,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    accumulate_grad_batches=config[\"accumulate_grad_batches\"],\n",
    "    overfit_batches=config[\"overfit_batch\"],\n",
    "    max_time=config[\"max_trainer_time_limit\"],\n",
    "    check_val_every_n_epoch=None,\n",
    "    val_check_interval=validate_every_n_steps,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,   \n",
    "    ckpt_path=(\n",
    "        f\"s3://deep-learning-personal-projects/{project_name}/{config['use_existing_run']}/last.ckpt\" \n",
    "        if config[\"use_existing_run\"]\n",
    "        else None\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.783Z"
    }
   },
   "outputs": [],
   "source": [
    "# test model denoising after training\n",
    "out = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\n",
    "plt.imshow(to_pil(inv_t(out.cpu()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e85271e-c6f7-41e1-86cb-216a74924336",
    "_uuid": "6aa28cef-6dc2-4443-abba-9228d6014f35",
    "execution": {
     "execution_failed": "2025-05-18T23:41:40.783Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if log_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (uv-env)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12251295,"sourceType":"datasetVersion","datasetId":7719381}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install dependencies if they aren't\n!pip install pytorch-lightning python-dotenv wandb s3fs==2025.5.1 --quiet\n\n# install from git\n!if [ -e ./diffusionpokemon ]; then rm -rf ./diffusionpokemon; fi\n!git clone https://github.com/LongDangHoang/DiffusionPokemon ./diffusionpokemon","metadata":{"_uuid":"29dcf535-3d39-45bc-920e-939b00811eeb","_cell_guid":"600fe82f-6d44-4212-8a4d-16141e1d2eff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"execution_failed":"2025-06-25T11:48:33.222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport sys\nsys.path.append(\"./diffusionpokemon\")\n\nimport re\nimport os\nimport torch\nimport torch.nn as nn\nimport datetime\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm.auto import tqdm\nfrom typing import List, Dict, Optional, Union, Tuple\nfrom pathlib import Path\n\nfrom diffusionpokemon.models.autoencoder_blocks import TimeEmbedding\nfrom diffusionpokemon.models.ddpm_unet import DDPMModel, DDPMUNet\nfrom diffusionpokemon.utils.callbacks import SampleCallback, DenoiseMidwaySampleCallback\nfrom diffusionpokemon.utils.datasets import create_train_test_datasets_from_path\n\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import v2\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, OneCycleLR\n\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import (\n    EarlyStopping, \n    StochasticWeightAveraging, \n    Callback, \n    ModelCheckpoint,\n    LearningRateMonitor,\n)\n\ntorch.manual_seed(314)\ntorch.cuda.manual_seed_all(314)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device}\")\nprint(f\"Using torch version: {torch.__version__}\")","metadata":{"_uuid":"4b133fd3-359b-4334-a65a-ff4a53d91d1f","_cell_guid":"cf8d14c8-7fb2-4bdd-8adb-c074cd08195b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Get data","metadata":{"_uuid":"f7d78736-dd66-4872-afb7-a12a6eadf9e7","_cell_guid":"ef17ba2d-be09-4599-8f85-dc8d586c3504","trusted":true}},{"cell_type":"code","source":"## Define hyperparameters\nfrom dotenv import load_dotenv\nimport json\n\nON_KAGGLE = False\nif not load_dotenv():\n    from kaggle_secrets import UserSecretsClient\n    os.environ[\"WANDB_API_KEY\"] = UserSecretsClient().get_secret(\"wandb_api\")\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = UserSecretsClient().get_secret(\"s3_aws_access_key\")\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = UserSecretsClient().get_secret(\"s3_aws_secret_access_key\")\n    ON_KAGGLE = True\n    \nimport wandb\n\nproject_name = PROJECT_NAME = \"diffusion-pokemon-lightning\"\n\nif config_json_filename := os.getenv(\"CONFIG_JSON_NAME\", None):\n    with open(config_json_filename, \"r\") as f:\n        config = json.load(f)\nelse:\n    config = dict(\n        batch_size = 16,\n        use_constant_lr = False,\n        lr = 1e-4,\n        num_epoch = 1_000,\n        dropout = 0.0,\n        overfit_batch = 0.0,\n        weight_decay = 0.0,\n        log_wandb = False,\n        transfer_from_run_s3_path = None,\n        resume_from_run = None,\n        use_augmentation = True,\n        accumulate_grad_batches = 8,\n        ddpm__n_steps = 1_000,\n        ddpm__input_image_channels = 3,\n        ddpm__input_image_size = 64,\n        unet__channels_mult = [1, 4, 4, 4, 4, 4],\n        unet__is_attn = [False, False, False, True, True, True],\n        unet__n_blocks = 12,\n        unet__hidden_dim = 64,\n        unet__use_conv_for_res_change = True,\n        is_finetune = False,\n        lr_scheduler__class = \"CosineAnnealingLR\",\n        max_trainer_time_limit = \"00:10:00:00\",\n    )\n\nfor k, v in config.items():\n    if k not in globals() or globals()[k] != v:\n        globals()[k] = v\n\n# start a new wandb run to track this script\nif log_wandb:\n    wandb.login()\n\n    if \"run\" not in globals():\n        run = wandb.init(\n            project=PROJECT_NAME,\n            id=resume_from_run,\n            resume=\"must\" if resume_from_run else None,\n            config={\n                **config,\n                \"mode\": \"offline\" if not log_wandb else \"online\"\n            }\n        )\n        assert run is not None\n\n# some common things\nto_pil = v2.ToPILImage()\n\ndef make_normaliser(num_channels=1):\n    return v2.Normalize([0.5]*num_channels, [1]*num_channels) # - 0.5 / 1\n\ndef make_inv_normaliser(num_channels=1):\n    return v2.Compose([transforms.Normalize([-0.5]*num_channels, [1]*num_channels), lambda x: torch.clip(x, 0, 1)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data augmentation transforms\ndata_aug_transforms = []\nif config[\"use_augmentation\"]:\n    data_aug_transforms = [\n        v2.RandomApply([v2.RandomResizedCrop(scale=(0.5, 0.8), size=config[\"ddpm__input_image_size\"])], p=0.1),\n        v2.RandomHorizontalFlip(p=0.5),\n        v2.RandomApply([v2.ColorJitter(contrast=0.1, hue=0.4)], p=0.3),\n    ]\n\ndataset_transform = transforms.Compose([\n    v2.Resize((config[\"ddpm__input_image_size\"], config[\"ddpm__input_image_size\"]), antialias=True),\n    *data_aug_transforms,\n    v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n    make_normaliser()\n])\n\n#  load datasets\ngenerator = torch.Generator(device=\"cpu\").manual_seed(413)\nall_dataset, train_dataset, test_dataset = create_train_test_datasets_from_path(\"/kaggle/input/pokemon-images-and-sprites\", transform=dataset_transform, use_transform_on_validation=True, random_seed=413)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inv_t = make_inv_normaliser()\n\nimg, path = test_dataset[10]\nplt.imshow(to_pil(inv_t(img)))\nprint(\"Image shape:\", img.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split train valid\nprint(f\"Train: {len(train_dataset)}, Valid: {len(test_dataset)}\")\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=True,\n    num_workers=os.cpu_count() - 2,\n    pin_memory=True,\n)\n\nvalid_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=False,\n    num_workers=os.cpu_count() - 2,\n    pin_memory=True,\n)\n\n# append steps_per_epoch\neffective_batch_size = batch_size * (1 if accumulate_grad_batches is None else accumulate_grad_batches)\nnum_train_examples = len(train_dataset) if overfit_batch == 0.0 else (overfit_batch * batch_size)\nsteps_per_epoch = math.ceil(num_train_examples / effective_batch_size)\nprint(\"Number of steps per epoch:\", f\"{steps_per_epoch}\")","metadata":{"_uuid":"89cc13cd-df1d-40a0-8945-7fde292b0580","_cell_guid":"61a814e1-bfdf-4c9e-97e1-4f71ab5134bc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Init model","metadata":{"_uuid":"94f988a5-0ba1-44d5-93bd-205107e315a1","_cell_guid":"cf723c8d-9711-4184-86b5-3b5580ca5ea1","trusted":true}},{"cell_type":"code","source":"optimizer_kwargs=dict(\n    use_constant_lr=config[\"use_constant_lr\"],\n    lr_scheduler__class=config[\"lr_scheduler__class\"],\n    lr=config[\"lr\"],\n    weight_decay=config[\"weight_decay\"],\n)\n\nif config[\"lr_scheduler__class\"] == \"CosineAnnealingLR\":\n    optimizer_kwargs[\"lr_sched_freq__step\"] = 1\n    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n        \"eta_min\": config[\"lr\"] / 100\n    }\nelif config[\"lr_scheduler_class\"] == \"ReduceLROnPlateau\":\n    optimizer_kwargs[\"lr_sched_freq__step\"] = steps_per_epoch\n    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n        \"eta_min\": config[\"lr\"] / 100\n    }\n\nmodel = DDPMUNet(\n    n_steps=config[\"ddpm__n_steps\"],\n    input_size=config[\"ddpm__input_image_size\"],\n    optimizers_kwarg=optimizer_kwargs,\n    eps_model_kwargs=dict(\n        n_blocks=config[\"unet__n_blocks\"], \n        n_channels=config[\"unet__hidden_dim\"],\n        channels_mult=config[\"unet__channels_mult\"], \n        is_attn=config[\"unet__is_attn\"],\n        res_block_dropout=config[\"dropout\"],\n        use_conv_for_res_change=config[\"unet__use_conv_for_res_change\"],\n        input_channels=config[\"ddpm__input_image_channels\"]\n    ),\n    is_finetune=False,   # we're not interested in finetuning for a while...\n).to(device)\n       \nprint(f\"Number of trainable params: {sum(p.numel() for p in model.eps_model.parameters() if p.requires_grad):,}\")\nprint(\"Model device:\", model.device)\n\nprint(optimizer_kwargs)","metadata":{"_uuid":"26f00ec1-2d45-4b81-b2a6-be664aafdaeb","_cell_guid":"0c680c92-bbe9-4b81-8eca-6e58f976467e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test model forward noising\nsample_img = train_dataset[13][0]\n\nnum_imgs = 10\nts = torch.as_tensor(np.round(np.linspace(0, model.n_steps-1, num_imgs)), dtype=torch.long).to(model.device)\nx = torch.unsqueeze(sample_img, 0).expand(num_imgs, *sample_img.shape).to(model.device)\ntrue_noise_e = torch.randn_like(x).to(model.device)\nnoised_x_t = model.noise_sample_at_timestep(x, ts, true_noise_e).cpu()\n\nfig, axs = plt.subplots(ncols=num_imgs, nrows=1, figsize=(num_imgs, 1))\nfor idx, ax in enumerate(axs):\n    ax.imshow(to_pil(inv_t(noised_x_t[idx])))\n    ax.grid(False)\n    ax.axis(False)\n    ax.set_title(f\"t={str(ts[idx].item())}\")\n    \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test model denoising\nout = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\nplt.imshow(to_pil(inv_t(out.cpu()[0])))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"_uuid":"9ada2b4e-0e48-44a8-a2fb-492d949ab702","_cell_guid":"251551c5-3a53-4a48-8d44-2ec0ce3d923a","trusted":true}},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(\n        monitor=\"train_loss__step\",\n        min_delta=-float('inf'),       # always accept\n        patience=5,\n        verbose=False,\n        check_finite=True,\n        mode=\"min\",\n    ),\n]\nlogger = None\n\nvalidate_every_n_steps = (steps_per_epoch * config[\"num_epoch\"]) // 100\n\nif config[\"log_wandb\"]:\n    logger = WandbLogger(project=project_name, prefix=\"CIFAR\")\n    \n    try:\n        logger.watch(model)\n    except ValueError as e:\n        if \"You can only call `wandb.watch` once per model.\" not in str(e):\n            raise e\n            \n    callbacks.append(\n        SampleCallback(\n            logger=logger, \n            inv_normaliser=make_inv_normaliser(), \n            every_n_steps=validate_every_n_steps, \n            batch_size=4,\n            input_channels=config[\"ddpm__input_image_channels\"],\n        )\n    )\n    callbacks.append(\n        DenoiseMidwaySampleCallback(\n            logger=logger, \n            seed_img_transformed=test_dataset[12][0],\n            noise_at_ts=[600, 300, 50],\n            every_n_steps=validate_every_n_steps,\n            inv_normaliser=make_inv_normaliser()\n        )\n    )\n\n    callbacks.append(\n        ModelCheckpoint(\n            dirpath=f\"s3://deep-learning-personal-projects/{project_name}/{run.id}/\",\n            filename=\"{epoch}-{step}--valid_loss={valid_loss__epoch:.3f}\",\n            monitor=\"valid_loss__epoch\",\n            save_last=True,\n            train_time_interval=datetime.timedelta(minutes=45),\n            save_top_k=1,\n            mode=\"min\",\n            enable_version_counter=False,\n        )\n    )\n\n    callbacks.append(LearningRateMonitor(logging_interval='step'))\n\n\nif config[\"transfer_from_run_s3_path\"] and not config[\"resume_from_run\"]:\n    model = DDPMUNet.load_from_checkpoint(config[\"transfer_from_run_s3_path\"])\n\ntrainer = Trainer(\n    accelerator=\"gpu\" if device == \"cuda\" else \"cpu\", \n    devices=1, \n    max_epochs=config[\"num_epoch\"],\n    log_every_n_steps=config[\"accumulate_grad_batches\"], # if accumulate, reduce logging frequency since we only do one optimisation step after X batches\n    precision=32,\n    logger=logger,\n    callbacks=callbacks,\n    accumulate_grad_batches=config[\"accumulate_grad_batches\"],\n    overfit_batches=config[\"overfit_batch\"],\n    max_time=config[\"max_trainer_time_limit\"],\n    check_val_every_n_epoch=None,\n    val_check_interval=validate_every_n_steps * config[\"accumulate_grad_batches\"],\n)\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.enabled   = True\ntorch.set_float32_matmul_precision(\"medium\")\n\ntrainer.fit(\n    model, \n    train_dataloaders=train_loader,\n    val_dataloaders=valid_loader,   \n    ckpt_path=(\n        f\"s3://deep-learning-personal-projects/{project_name}/{config['resume_from_run']}/last.ckpt\" \n        if config[\"resume_from_run\"]\n        else None\n    )\n)","metadata":{"_uuid":"5bcc4915-e866-4bf9-bbec-fc4526654dc3","_cell_guid":"1472c65f-f076-4f5a-bc2c-ef60d0d3e958","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test model denoising after training\nout = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\nplt.imshow(to_pil(inv_t(out.cpu()[0])))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if log_wandb:\n    wandb.finish()","metadata":{"_uuid":"6aa28cef-6dc2-4443-abba-9228d6014f35","_cell_guid":"1e85271e-c6f7-41e1-86cb-216a74924336","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null}]}
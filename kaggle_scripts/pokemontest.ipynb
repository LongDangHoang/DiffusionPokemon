{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf8d14c8-7fb2-4bdd-8adb-c074cd08195b",
    "_uuid": "4b133fd3-359b-4334-a65a-ff4a53d91d1f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/workspace/DiffusionPokemonProject\")  # for vm with workspace folder, (e.g. vast ai, lambda)\n",
    "sys.path.append(\"./diffusionpokemon\")  # for kaggle or ec2\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Optional, Union, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusionpokemon.models.autoencoder_blocks import TimeEmbedding\n",
    "from diffusionpokemon.models.ddpm_unet import DDPMModel, DDPMUNet\n",
    "from diffusionpokemon.utils.callbacks import SampleCallback, DenoiseMidwaySampleCallback\n",
    "from diffusionpokemon.utils.datasets import create_train_test_datasets_from_path\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping, \n",
    "    StochasticWeightAveraging, \n",
    "    Callback, \n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "\n",
    "torch.manual_seed(314)\n",
    "torch.cuda.manual_seed_all(314)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")\n",
    "print(f\"Using torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef17ba2d-be09-4599-8f85-dc8d586c3504",
    "_uuid": "f7d78736-dd66-4872-afb7-a12a6eadf9e7",
    "trusted": true
   },
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Define hyperparameters\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "ON_KAGGLE = False\n",
    "if not load_dotenv():\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    os.environ[\"WANDB_API_KEY\"] = UserSecretsClient().get_secret(\"wandb_api\")\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = UserSecretsClient().get_secret(\"s3_aws_access_key\")\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = UserSecretsClient().get_secret(\"s3_aws_secret_access_key\")\n",
    "    ON_KAGGLE = True\n",
    "    \n",
    "import wandb\n",
    "\n",
    "project_name = PROJECT_NAME = \"diffusion-pokemon-lightning\"\n",
    "\n",
    "if config_json_filename := os.getenv(\"CONFIG_JSON_NAME\", None):\n",
    "    with open(config_json_filename, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "else:\n",
    "    config = dict(\n",
    "        batch_size = 16,\n",
    "        use_constant_lr = False,\n",
    "        lr = 1e-4,\n",
    "        num_epoch = 1_000,\n",
    "        dropout = 0.0,\n",
    "        overfit_batch = 0.0,\n",
    "        weight_decay = 0.0,\n",
    "        log_wandb = False,\n",
    "        transfer_from_run_s3_path = None,\n",
    "        resume_from_run = None,\n",
    "        use_augmentation = True,\n",
    "        accumulate_grad_batches = 8,\n",
    "        ddpm__n_steps = 1_000,\n",
    "        ddpm__input_image_channels = 3,\n",
    "        ddpm__input_image_size = 64,\n",
    "        unet__channels_mult = [1, 4, 4, 2, 2, 2],\n",
    "        unet__is_attn = [False, False, False, True, True, True],\n",
    "        unet__n_blocks = 12,\n",
    "        unet__hidden_dim = 256,\n",
    "        unet__use_conv_for_res_change = True,\n",
    "        is_finetune = False,\n",
    "        lr_scheduler__class = \"CosineAnnealingLR\",\n",
    "        max_trainer_time_limit = \"00:10:00:00\",\n",
    "    )\n",
    "\n",
    "for k, v in config.items():\n",
    "    if k not in globals() or globals()[k] != v:\n",
    "        globals()[k] = v\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "if config[\"log_wandb\"]:\n",
    "    wandb.login()\n",
    "\n",
    "    if \"run\" not in globals():\n",
    "        run = wandb.init(\n",
    "            project=PROJECT_NAME,\n",
    "            id=config[\"resume_from_run\"],\n",
    "            resume=\"must\" if config[\"resume_from_run\"] else None,\n",
    "            config={\n",
    "                **{k: v for k, v in config.items() if k not in (\"log_wandb\", \"resume_from_run\", \"max_trainer_time_limit\"),\n",
    "                \"mode\": \"offline\" if not config[\"log_wandb\"] else \"online\"\n",
    "            }\n",
    "        )\n",
    "        assert run is not None\n",
    "\n",
    "# some common things\n",
    "to_pil = v2.ToPILImage()\n",
    "\n",
    "def make_normaliser(num_channels=1):\n",
    "    return v2.Normalize([0.5]*num_channels, [1]*num_channels) # - 0.5 / 1\n",
    "\n",
    "def make_inv_normaliser(num_channels=1):\n",
    "    return v2.Compose([transforms.Normalize([-0.5]*num_channels, [1]*num_channels), lambda x: torch.clip(x, 0, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data augmentation transforms\n",
    "data_aug_transforms = []\n",
    "if config[\"use_augmentation\"]:\n",
    "    data_aug_transforms = [\n",
    "        v2.RandomApply([v2.RandomResizedCrop(scale=(0.5, 0.8), size=config[\"ddpm__input_image_size\"])], p=0.1),\n",
    "        v2.RandomHorizontalFlip(p=0.5),\n",
    "        v2.RandomApply([v2.ColorJitter(contrast=0.1, hue=0.4)], p=0.3),\n",
    "    ]\n",
    "\n",
    "dataset_transform = transforms.Compose([\n",
    "    v2.Resize((config[\"ddpm__input_image_size\"], config[\"ddpm__input_image_size\"]), antialias=True),\n",
    "    *data_aug_transforms,\n",
    "    v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]),\n",
    "    make_normaliser()\n",
    "])\n",
    "\n",
    "#  load datasets\n",
    "generator = torch.Generator(device=\"cpu\").manual_seed(413)\n",
    "all_dataset, train_dataset, test_dataset = create_train_test_datasets_from_path(\"/kaggle/input/pokemon-images-and-sprites\", transform=dataset_transform, use_transform_on_validation=True, random_seed=413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inv_t = make_inv_normaliser()\n",
    "\n",
    "img, path = test_dataset[10]\n",
    "plt.imshow(to_pil(inv_t(img)))\n",
    "print(\"Image shape:\", img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61a814e1-bfdf-4c9e-97e1-4f71ab5134bc",
    "_uuid": "89cc13cd-df1d-40a0-8945-7fde292b0580",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# split train valid\n",
    "print(f\"Train: {len(train_dataset)}, Valid: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=os.cpu_count() - 2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=os.cpu_count() - 2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# append steps_per_epoch\n",
    "effective_batch_size = config[\"batch_size\"] * (1 if config[\"accumulate_grad_batches\"] is None else config[\"accumulate_grad_batches\"])\n",
    "num_train_examples = len(train_dataset) if config[\"overfit_batch\"] == 0.0 else (config[\"overfit_batch\"] * config[\"batch_size\"])\n",
    "steps_per_epoch = math.ceil(num_train_examples / effective_batch_size)\n",
    "print(\"Number of steps per epoch:\", f\"{steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf723c8d-9711-4184-86b5-3b5580ca5ea1",
    "_uuid": "94f988a5-0ba1-44d5-93bd-205107e315a1",
    "trusted": true
   },
   "source": [
    "# Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0c680c92-bbe9-4b81-8eca-6e58f976467e",
    "_uuid": "26f00ec1-2d45-4b81-b2a6-be664aafdaeb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer_kwargs=dict(\n",
    "    use_constant_lr=config[\"use_constant_lr\"],\n",
    "    lr_scheduler__class=config[\"lr_scheduler__class\"],\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "if config[\"lr_scheduler__class\"] == \"CosineAnnealingLR\":\n",
    "    optimizer_kwargs[\"lr_sched_freq__step\"] = 1\n",
    "    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n",
    "        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n",
    "        \"eta_min\": config[\"lr\"] / 100\n",
    "    }\n",
    "elif config[\"lr_scheduler_class\"] == \"ReduceLROnPlateau\":\n",
    "    optimizer_kwargs[\"lr_sched_freq__step\"] = steps_per_epoch\n",
    "    optimizer_kwargs[\"lr_scheduler__kwargs\"] = {\n",
    "        \"T_max\": config[\"num_epoch\"] * steps_per_epoch,\n",
    "        \"eta_min\": config[\"lr\"] / 100\n",
    "    }\n",
    "\n",
    "model = DDPMUNet(\n",
    "    n_steps=config[\"ddpm__n_steps\"],\n",
    "    input_size=config[\"ddpm__input_image_size\"],\n",
    "    optimizers_kwarg=optimizer_kwargs,\n",
    "    eps_model_kwargs=dict(\n",
    "        n_blocks=config[\"unet__n_blocks\"], \n",
    "        n_channels=config[\"unet__hidden_dim\"],\n",
    "        channels_mult=config[\"unet__channels_mult\"], \n",
    "        is_attn=config[\"unet__is_attn\"],\n",
    "        res_block_dropout=config[\"dropout\"],\n",
    "        use_conv_for_res_change=config[\"unet__use_conv_for_res_change\"],\n",
    "        input_channels=config[\"ddpm__input_image_channels\"]\n",
    "    ),\n",
    "    is_finetune=False,   # we're not interested in finetuning for a while...\n",
    ").to(device)\n",
    "       \n",
    "print(f\"Number of trainable params: {sum(p.numel() for p in model.eps_model.parameters() if p.requires_grad):,}\")\n",
    "print(\"Model device:\", model.device)\n",
    "\n",
    "print(optimizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test model forward noising\n",
    "sample_img = train_dataset[13][0]\n",
    "\n",
    "num_imgs = 10\n",
    "ts = torch.as_tensor(np.round(np.linspace(0, model.n_steps-1, num_imgs)), dtype=torch.long).to(model.device)\n",
    "x = torch.unsqueeze(sample_img, 0).expand(num_imgs, *sample_img.shape).to(model.device)\n",
    "true_noise_e = torch.randn_like(x).to(model.device)\n",
    "noised_x_t = model.noise_sample_at_timestep(x, ts, true_noise_e).cpu()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=num_imgs, nrows=1, figsize=(num_imgs, 1))\n",
    "for idx, ax in enumerate(axs):\n",
    "    ax.imshow(to_pil(inv_t(noised_x_t[idx])))\n",
    "    ax.grid(False)\n",
    "    ax.axis(False)\n",
    "    ax.set_title(f\"t={str(ts[idx].item())}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test model denoising\n",
    "out = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\n",
    "plt.imshow(to_pil(inv_t(out.cpu()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "251551c5-3a53-4a48-8d44-2ec0ce3d923a",
    "_uuid": "9ada2b4e-0e48-44a8-a2fb-492d949ab702",
    "trusted": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1472c65f-f076-4f5a-bc2c-ef60d0d3e958",
    "_uuid": "5bcc4915-e866-4bf9-bbec-fc4526654dc3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"train_loss__step\",\n",
    "        min_delta=-float('inf'),       # always accept\n",
    "        patience=5,\n",
    "        verbose=False,\n",
    "        check_finite=True,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "]\n",
    "logger = None\n",
    "\n",
    "validate_every_n_steps = (steps_per_epoch * config[\"num_epoch\"]) // 100\n",
    "\n",
    "if config[\"log_wandb\"]:\n",
    "    logger = WandbLogger(project=project_name, prefix=\"POKEMON\")\n",
    "    \n",
    "    try:\n",
    "        logger.watch(model)\n",
    "    except ValueError as e:\n",
    "        if \"You can only call `wandb.watch` once per model.\" not in str(e):\n",
    "            raise e\n",
    "            \n",
    "    callbacks.append(\n",
    "        SampleCallback(\n",
    "            logger=logger, \n",
    "            inv_normaliser=make_inv_normaliser(), \n",
    "            every_n_steps=validate_every_n_steps, \n",
    "            batch_size=4,\n",
    "            input_channels=config[\"ddpm__input_image_channels\"],\n",
    "        )\n",
    "    )\n",
    "    callbacks.append(\n",
    "        DenoiseMidwaySampleCallback(\n",
    "            logger=logger, \n",
    "            seed_img_transformed=test_dataset[12][0],\n",
    "            noise_at_ts=[600, 300, 50],\n",
    "            every_n_steps=validate_every_n_steps,\n",
    "            inv_normaliser=make_inv_normaliser()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    callbacks.append(\n",
    "        ModelCheckpoint(\n",
    "            dirpath=f\"s3://deep-learning-personal-projects/{project_name}/{run.id}/\",\n",
    "            filename=\"{epoch}-{step}--valid_loss={valid_loss__epoch:.3f}\",\n",
    "            monitor=\"valid_loss__epoch\",\n",
    "            save_last=True,\n",
    "            train_time_interval=datetime.timedelta(minutes=45),\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "            enable_version_counter=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    callbacks.append(LearningRateMonitor(logging_interval='step'))\n",
    "\n",
    "\n",
    "if config[\"transfer_from_run_s3_path\"] and not config[\"resume_from_run\"]:\n",
    "    model = DDPMUNet.load_from_checkpoint(config[\"transfer_from_run_s3_path\"])\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\" if device == \"cuda\" else \"cpu\", \n",
    "    devices=1, \n",
    "    max_epochs=config[\"num_epoch\"],\n",
    "    log_every_n_steps=config[\"accumulate_grad_batches\"], # if accumulate, reduce logging frequency since we only do one optimisation step after X batches\n",
    "    precision=32,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    accumulate_grad_batches=config[\"accumulate_grad_batches\"],\n",
    "    overfit_batches=config[\"overfit_batch\"],\n",
    "    max_time=config[\"max_trainer_time_limit\"],\n",
    "    check_val_every_n_epoch=None,\n",
    "    val_check_interval=validate_every_n_steps * config[\"accumulate_grad_batches\"],\n",
    ")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled   = True\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=valid_loader,   \n",
    "    ckpt_path=(\n",
    "        f\"s3://deep-learning-personal-projects/{project_name}/{config['resume_from_run']}/last.ckpt\" \n",
    "        if config[\"resume_from_run\"]\n",
    "        else None\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test model denoising after training\n",
    "out = model.sample(batch_size=1, input_channels=config[\"ddpm__input_image_channels\"])\n",
    "plt.imshow(to_pil(inv_t(out.cpu()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e85271e-c6f7-41e1-86cb-216a74924336",
    "_uuid": "6aa28cef-6dc2-4443-abba-9228d6014f35",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config[\"log_wandb\"]:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7719381,
     "sourceId": 12251295,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
